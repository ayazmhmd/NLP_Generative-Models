{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Very Basic Implementation of Generative Model using Bigram LSTM Model\n",
        "- Defined Everything custom so that it would help you to understand the basic concept behind generative models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training Corpus You can load your own dataset too"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "es5c9ZFOsAyc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import defaultdict\n",
        "\n",
        "corpus = [\n",
        "    \"I like natural language processing\",\n",
        "    \"I like deep learning\",\n",
        "    \"I like PyTorch\"\n",
        "    \"I love to work on coding\"\n",
        "]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Custom Simple tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "tokens = [sentence.lower().split() for sentence in corpus]\n",
        "\n",
        "# Create a vocabulary\n",
        "vocab = set([word for sentence in tokens for word in sentence])\n",
        "\n",
        "# Create word-to-index and index-to-word mappings\n",
        "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
        "ix_to_word = {i: word for i, word in enumerate(vocab)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Bigram counts for each word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlinqtHEsDXj",
        "outputId": "12d45b0c-448e-4597-e0cc-81d0d2e7a955"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'i': {'like': 3}, 'like': {'natural': 1, 'deep': 1, 'pytorchi': 1}, 'natural': {'language': 1}, 'language': {'processing': 1}, 'deep': {'learning': 1}, 'pytorchi': {'love': 1}, 'love': {'to': 1}, 'to': {'work': 1}, 'work': {'on': 1}, 'on': {'coding': 1}}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "bigram_counts = {}\n",
        "\n",
        "for sentence in tokens:\n",
        "\n",
        "    for i in range(1, len(sentence)):\n",
        "\n",
        "        current_word = sentence[i]\n",
        "        previous_word = sentence[i - 1]\n",
        "\n",
        "        if previous_word not in bigram_counts:\n",
        "            bigram_counts[previous_word] = {}\n",
        "\n",
        "        if current_word not in bigram_counts[previous_word]:\n",
        "            bigram_counts[previous_word][current_word] = 0\n",
        "\n",
        "        bigram_counts[previous_word][current_word] += 1\n",
        "\n",
        "\n",
        "print(bigram_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Bigram Model with LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "bC7oe_0GtqQ8"
      },
      "outputs": [],
      "source": [
        "\n",
        "class BigramLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size,embedding_dim,hidden_dim,):\n",
        "        super(BigramLSTM, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_sim=embedding_dim\n",
        "        self.hidden_dim=hidden_dim\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embeds = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(embeds)\n",
        "        output = self.fc(lstm_out[:, -1, :])\n",
        "        return output\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.01\n",
        "num_epochs = 100\n",
        "embedding_dim=32\n",
        "hidden_dim=16\n",
        "# Initialize the model and optimizer\n",
        "model = BigramLSTM(len(vocab),embedding_dim,hidden_dim)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7aSuy1Mt20z",
        "outputId": "e747c7e1-0c25-4f1b-cda5-3ffffc03917e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_items([('i', {'like': 3}), ('like', {'natural': 1, 'deep': 1, 'pytorchi': 1}), ('natural', {'language': 1}), ('language', {'processing': 1}), ('deep', {'learning': 1}), ('pytorchi', {'love': 1}), ('love', {'to': 1}), ('to', {'work': 1}), ('work', {'on': 1}), ('on', {'coding': 1})])"
            ]
          },
          "execution_count": 97,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bigram_counts.items()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcBNe9Eut_Xs",
        "outputId": "b1c9f0c4-732f-4bdb-87b3-f934c9f05377"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "i\n",
            "{'like': 3}\n",
            "like\n",
            "{'natural': 1, 'deep': 1, 'pytorchi': 1}\n",
            "natural\n",
            "{'language': 1}\n",
            "language\n",
            "{'processing': 1}\n",
            "deep\n",
            "{'learning': 1}\n",
            "pytorchi\n",
            "{'love': 1}\n",
            "love\n",
            "{'to': 1}\n",
            "to\n",
            "{'work': 1}\n",
            "work\n",
            "{'on': 1}\n",
            "on\n",
            "{'coding': 1}\n"
          ]
        }
      ],
      "source": [
        "for context_word_ix, next_word_ixs in bigram_counts.items():\n",
        "    print(context_word_ix)\n",
        "    print(next_word_ixs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PoHT3j3GsE0j",
        "outputId": "96184118-01cd-4d00-836d-54f39e02f57f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10, Loss: 6.3482209369540215\n",
            "Epoch 20, Loss: 4.003192204982042\n",
            "Epoch 30, Loss: 3.676830952987075\n",
            "Epoch 40, Loss: 3.5573324924334884\n",
            "Epoch 50, Loss: 3.497047302313149\n",
            "Epoch 60, Loss: 3.461038926616311\n",
            "Epoch 70, Loss: 3.4393726114649326\n",
            "Epoch 80, Loss: 3.4246906868647784\n",
            "Epoch 90, Loss: 3.41412171931006\n",
            "Epoch 100, Loss: 3.406180937658064\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for context_word_ix, next_word_ixs in bigram_counts.items():\n",
        "        context_tensor = torch.tensor([[word_to_ix[context_word_ix]]])\n",
        "        for next_word_ix, count in next_word_ixs.items():\n",
        "            optimizer.zero_grad()\n",
        "            output = model(context_tensor)\n",
        "            loss = nn.CrossEntropyLoss()(output, torch.tensor([word_to_ix[next_word_ix]]))\n",
        "            total_loss += loss.item()\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "    # Print loss every 10 epochs\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'Epoch {epoch+1}, Loss: {total_loss}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Simple Testing of Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HI4QdEW5yz3T",
        "outputId": "b2f6b35f-623f-4524-e08d-228780f9ec63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Given the context 'i like deep', the model predicts the next word is 'learning'\n"
          ]
        }
      ],
      "source": [
        "context = \"i like deep\"\n",
        "context_tokens = context.split()\n",
        "context_tensor = torch.tensor([[word_to_ix[word] for word in context_tokens]])\n",
        "output = model(context_tensor)\n",
        "next_word_ix = torch.argmax(output).item()\n",
        "next_word = ix_to_word[next_word_ix]\n",
        "print(f\"Given the context '{context}', the model predicts the next word is '{next_word}'\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
